{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12012727 王铎磊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380438e",
   "metadata": {},
   "source": [
    "## LAB Assignment  \n",
    "This lab introduces classical machine learning algorithms, decision trees (DTs) and their ensemble learning (e.g., Random Forests). Decision trees are important non-parameter learning methods. Although DTs are simple and limited, they still can achieve excellent performance using ensemble learning schemes.\n",
    "\n",
    "For this lab assignment, we'll use the algorithms we've learned today to fit the model and evaluate the model’s prediction performance. The scikit-learn package will be used to save your time.\n",
    "\n",
    "\n",
    "### Decision tree\n",
    "- <font size=4>Step 1. load iris dataset </font>\n",
    "\n",
    "Datasets: First, we load the scikit-learn iris toy dataset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c54139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae61bd",
   "metadata": {},
   "source": [
    "- <font size=4> Step 2. Define the features and the target </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7821f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f214de",
   "metadata": {},
   "source": [
    "- <font size=4> Step 3. Visualization </font>\n",
    "  \n",
    "    We need to use proper visualization methods to have an intuitive understanding.\n",
    "\n",
    "    For visualization, only the last 2 attributes are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55e229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[y==0, 2], X[y==0, 3])\n",
    "plt.scatter(X[y==1, 2], X[y==1, 3])\n",
    "plt.scatter(X[y==2, 2], X[y==2, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323138bd",
   "metadata": {},
   "source": [
    "- <font size=4> Step 4. Preprocessing data </font>\n",
    "Please check whether the data needs to be preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd936db9",
   "metadata": {},
   "source": [
    "数据集中没有缺失数据，而且每一个 Feature 都是连续的数值型数据，因此不需要额外的数据处理."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a4331",
   "metadata": {},
   "source": [
    "- <font size=4> Step 5. Split the dataset into train and test sets </font>\n",
    "  \n",
    " Now we divide the whole dataset into a training set and a test set using the the scikit-learn model_selection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917b0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d565df",
   "metadata": {},
   "source": [
    "- <font size=4> Step 6. Explore the model parameters </font>\n",
    "  \n",
    "Decision trees are quite easy to use, but they are prone to overfit the training data. Actually almost all the non-parameter learning methods suffer from this problem. We can use pruning to optimize our trained decision trees; we can also adjust the super parameters to avoid overfitting.\n",
    "\n",
    "    The decision tree model  given by the `SkLearn`  is as follows:\n",
    "\n",
    "    ```python\n",
    "    DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
    "    ```\n",
    "    There are so many arguments and they are all helpful in adjusting the algorithm parameters to achieve the balance between bias and variance.  \n",
    "    Adjust these parameters: `criterion`,`max_depth`, `min_samples_leaf`,  `min_samples_split` , `max_leaf_nodes `,`min_impurity_split `\n",
    "    and explain how it affects the bias and variance of the classification results. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36708436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9500000000000001 {'tree__criterion': 'gini', 'tree__max_depth': 2, 'tree__max_leaf_nodes': 3, 'tree__min_impurity_decrease': 0.0, 'tree__min_samples_leaf': 1}\n",
      "0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = Pipeline([(\"tree\", DecisionTreeClassifier())])\n",
    "\n",
    "DecisionTreeClassifier\n",
    "\n",
    "params = {\"tree__criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "          \"tree__max_depth\": range(1, 5),\n",
    "          \"tree__min_samples_leaf\": range(1, 10),\n",
    "          \"tree__max_leaf_nodes\": range(2, 6),\n",
    "          \"tree__min_impurity_decrease\": [x/10 for x in range(5)]}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=params, cv=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_score_, grid_search.best_params_)\n",
    "print(grid_search.score(X_train, y_train))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c2d7a",
   "metadata": {},
   "source": [
    "    Finally, select the best set of parameters for the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a8954",
   "metadata": {},
   "source": [
    "- <font size=4> Step 7. Use the model of your choice on the test set </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8463770",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, max_leaf_nodes=4, min_impurity_decrease=0.0, min_samples_leaf=2)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ce1a6",
   "metadata": {},
   "source": [
    "- <font size=4> Step 8. Evaluate the model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, r2_score\n",
    "\n",
    "def getStdBias(y_true:np.array, y_pred:np.array):\n",
    "    return np.std(y_true - y_pred)\n",
    "    \n",
    "\n",
    "def getVariance(y_true:np.array, y_pred:np.array):\n",
    "    return np.var(np.square(y_true - y_pred))\n",
    "        \n",
    "        \n",
    "def printScores(model):\n",
    "    print(\"Accuracy in train set:\", accuracy_score(y_train, model.predict(X_train)))\n",
    "    print(\"Accuracy in test set:\", accuracy_score(y_test, model.predict(X_test)))\n",
    "    print(\"f1-score in test set:\", f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "    print(\"r2-score in test set:\", r2_score(y_test, y_pred))\n",
    "    print(\"Bias in train-set:\", getStdBias(y_train, model.predict(X_train)))\n",
    "    print(\"Bias in test-set:\", getStdBias(y_test, model.predict(X_test)))\n",
    "    print(\"Variance in train-set:\", getVariance(y_train, model.predict(X_train)))\n",
    "    print(\"Variance in test-set:\", getVariance(y_test, model.predict(X_test)))\n",
    "    print(\"Cross-Validation-Score:\", np.average(cross_val_score(model, X, y, cv = 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfe1a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in train set: 0.9583333333333334\n",
      "Accuracy in test set: 0.9666666666666667\n",
      "f1-score in test set: 0.9665634674922601\n",
      "r2-score in test set: 0.9516908212560387\n",
      "Bias in train-set: 0.2025874296857203\n",
      "Bias in test-set: 0.17950549357115012\n",
      "Variance in train-set: 0.03993055555555557\n",
      "Variance in test-set: 0.032222222222222215\n",
      "Cross-Validation-Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, r2_score\n",
    "\n",
    "printScores(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb79fce",
   "metadata": {},
   "source": [
    "- <font size=4> Step 9. Visual decision boundary and generated decision tree </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04d5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plot_decision_regions(X_train, y_train, clf=tree)\t\n",
    "plt.title('Decision Tree Classifier')\t\n",
    "plt.show()\t\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf4b92",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "In this section, you are required to use random forests for classification. Thus, in `scikit-learn`, there are two ways to implement a random forset, from the Bagging view and from the RF view.<br>\n",
    "Classify `iris`  using `BaggingClassifier( )` and `RandomForestClassifier( )` respectively, \n",
    "\n",
    "- <font color=blue >**RF view:**</font> we construct a RF class directly.\n",
    "  \n",
    "```python\n",
    "# Use Random Forest directly\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=300,\n",
    "                                random_state=666, # random attributes subset\n",
    "                                oob_score=True,\n",
    "                                # n_jobs=-1\n",
    "                               )\n",
    "rf_clf.fit(X,y)\n",
    "```\n",
    "\n",
    "- <font color=blue face=雅黑>**Bagging view:**</font>  we use the bagging algorithm with a number of base learning algorithms of decision trees.\n",
    "  \n",
    "```python\n",
    "# Use Random Forest from Bagging view\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                                n_estimators=300,\n",
    "                                max_samples=300,\n",
    "                                bootstrap=True, # using bootstrap sampling method\n",
    "                                oob_score=True, # use oob data for scoring\n",
    "                                # n_jobs=-1 # use paralell computing\n",
    "                               )\n",
    "bagging_clf.fit(X,y)\n",
    "```\n",
    "\n",
    "- Compare the performances of two  methods, and select different parameters for model  and evaluate the model using bias and variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c3b3006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9666666666666667\n",
      "f1-score in test set: 0.9665634674922601\n",
      "r2-score in test set: 0.9516908212560387\n",
      "Bias in train-set: 0.0\n",
      "Bias in test-set: 0.17950549357115012\n",
      "Variance in train-set: 0.0\n",
      "Variance in test-set: 0.032222222222222215\n",
      "Cross-Validation-Score: 0.96\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9666666666666667\n",
      "f1-score in test set: 0.9665634674922601\n",
      "r2-score in test set: 0.9516908212560387\n",
      "Bias in train-set: 0.0\n",
      "Bias in test-set: 0.17950549357115012\n",
      "Variance in train-set: 0.0\n",
      "Variance in test-set: 0.032222222222222215\n",
      "Cross-Validation-Score: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100,\n",
    "                                random_state=57\n",
    "                                # n_jobs=-1\n",
    "                               )\n",
    "rf_clf.fit(X_train, y_train)\n",
    "printScores(rf_clf)\n",
    "\n",
    "print()\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                                n_estimators=100,\n",
    "                                bootstrap=True, # using bootstrap sampling method\n",
    "                                oob_score=True, # use oob data for scoring\n",
    "                                # n_jobs=-1 # use paralell computing\n",
    "                               )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "printScores(bag_clf)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507135b",
   "metadata": {},
   "source": [
    "### Other ensemble learning\n",
    "For classification, we have many models to choose . Please don't just pick a model to train and say it's good enough. We need to select models based on some metrics, such as choosing models with low bias and low variance.\n",
    "\n",
    "In this part, you are required  to use `AdaBoost` and `Gradient boosting`.Compare their performances with decision tree and random forest, and finally select the best model  and the optimal  parameters for iris classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd238229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in train set: 0.975\n",
      "Accuracy in test set: 0.9666666666666667\n",
      "f1-score in test set: 0.9665634674922601\n",
      "r2-score in test set: 0.9516908212560387\n",
      "Bias in train-set: 0.15789412767913683\n",
      "Bias in test-set: 0.17950549357115012\n",
      "Variance in train-set: 0.024374999999999997\n",
      "Variance in test-set: 0.032222222222222215\n",
      "Cross-Validation-Score: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier()\n",
    "ada_clf.fit(X_train, y_train)\n",
    "printScores(ada_clf)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9666666666666667\n",
      "f1-score in test set: 0.9665634674922601\n",
      "r2-score in test set: 0.9516908212560387\n",
      "Bias in train-set: 0.0\n",
      "Bias in test-set: 0.17950549357115012\n",
      "Variance in train-set: 0.0\n",
      "Variance in test-set: 0.032222222222222215\n",
      "Cross-Validation-Score: 0.9600000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc_clf = GradientBoostingClassifier()\n",
    "\n",
    "gbc_clf.fit(X_train, y_train)\n",
    "printScores(gbc_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ada32b",
   "metadata": {},
   "source": [
    "\n",
    "<font size=4>Hint: About how to select models and parameters:</font>\n",
    "- Select model using cross validation. Compare the scores in the training set and the validation set. If they are good enough, use the model in the test set.\n",
    "- Calculate the bias and variance of each model to further analyze your chosen model.\n",
    "- Select parameters using cross validation\n",
    "  \n",
    "### Questions:\n",
    "(1) Can decision trees and random forests be used for unsupervised clustering or data dimension reduction? Why?\n",
    "\n",
    "(2) What are the strengths of the decision tree/random forest methods; when do they perform well?\n",
    "\n",
    "(3) What are the weaknesses of the decision tree/random forest methods; when do they perform poorly?\n",
    "\n",
    "(4) What makes the decision tree/random forest a good candidate for the classification/regression problem, if you have enough knowledge about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 不可以，算法需要提供标签，因此适用于监督学习.\n",
    "\n",
    "2. 决策树模型可以用于小数据集，复杂度较小，速度较快；随机森林相比之下较好的解决了过拟合问题，并且能有效降低模型的方差，适用于高维数据，对于数据集的选择无需额外操作，泛化能力强.\n",
    "\n",
    "3. 决策树容易过拟合，泛化能力较差，对于非分类问题不好，树结构简单，可理解程度高；随机森林计算量更大，计算代价更多，对于回归问题不好用，森林像是黑箱，可理解程度低.\n",
    "\n",
    "4. 如果样本数据量小，且是分类问题，有明显的特征，可以考虑决策树模型.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e916b74",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Decision trees are prone to overfitting, but random forest algorithm prevents overfitting.\n",
    "- Random forest algorithm is comparatively time-consuming, whereas decision tree algorithm gives fast results.\n",
    "- There are many arguments for either base decision trees or the whole ensemble algorithm.  A good ensemble algorithm should make sure that base ones are both accurate and diversified.  So it is better to get a set of good enough base tree parameters before training the ensemble learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0c458",
   "metadata": {},
   "source": [
    "## References\n",
    "https://scikit-learn.org/stable/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
