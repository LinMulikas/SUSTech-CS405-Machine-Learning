{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB12 tutorial for Machine Learning <br > Clustering with GMM\n",
    "> The document description are designed by JIa Yanhong in 2022. Nov. 21th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Understand GMM clustering algorithm theory\n",
    "- Implement the GMM clustering algorithm  from scratch in python\n",
    "- Complete the LAB assignment and submit it to BB or sakai.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of k-means Clustering\n",
    "\n",
    "The k-means clustering concept sounds pretty great, right? It’s simple to understand, relatively easy to implement, and can be applied in quite a number of use cases. But there are certain drawbacks and limitations that we need to be aware of. <font color=red>K-means often doesn't work when clusters are not round shaped</font>\n",
    "\n",
    "First, KMeans doesn't put data points that are far away from each other into the same cluster, even when they obviously should be because they underly some obvious structure like points on a line, for example.\n",
    "\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/two_lines.png\"  width=400 align=center /></div>\n",
    "\n",
    "\n",
    "\n",
    "Second, KMeans performs poorly for complicated geometric shapes such as the moons and circles shown below.\n",
    "\n",
    "\n",
    "<div  align=\"center\"> \n",
    "<img src=\"images/noisy_moons_with_true_output.png\"  width=400 align=center />\n",
    "<img src=\"images/noisy_circles_with_true_output.png\"  width=400 align=center />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "In addition,k-means doesn't work when clusters are may overlap.\n",
    "<div  align=\"center\"> <img src=\"images/image-20221121170553059.png\"  width=200 align=center /></div>\n",
    "\n",
    "\n",
    "Hence, we need a different way to assign clusters to the data points.  So instead of using a distance-based model, we will now use a distribution-based model.  And that is where `Gaussian Mixture Models` come into this lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixture model (GMM)\n",
    "\n",
    "Gaussian Mixture Models (GMMs) assume that there are a certain number of Gaussian distributions, and each of these distributions represent a cluster.\n",
    "\n",
    "**Gaussian Mixture Models are probabilistic models and use the soft clustering approach for distributing the points in different clusters.** \n",
    "\n",
    " Let us take an example that will make it easier to understand.\n",
    "\n",
    "Here, we have three clusters that are denoted by three colors – Blue, Green, and Cyan. Let’s take the data point highlighted in red. The probability of this point being a part of the blue cluster is 1, while the probability of it being a part of the green or cyan clusters is 0.\n",
    "\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/Screenshot-from-2019-10-21-12-52-06.png\"  width=400 align=center /></div>\n",
    "Now, consider another point – somewhere in between the blue and cyan (highlighted in the below figure). The probability that this point is a part of cluster green is 0, right? And the probability that this belongs to blue and cyan is 0.2 and 0.8 respectively.\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/Screenshot-from-2019-10-21-12-53-29.png\"  width=400 align=center /></div>\n",
    "\n",
    "\n",
    "Gaussian Mixture Models use the soft clustering technique for assigning data points to Gaussian distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gaussian Distribution\n",
    "\n",
    "In a one dimensional space, the **probability density function** of a Gaussian distribution is given by:\n",
    "\n",
    "\n",
    "\n",
    "$$\\mathcal{N}(X|\\mu, \\sigma)=\\frac{1}{{\\sigma \\sqrt {2\\pi } }}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "where μ is the mean and $\\sigma^{2}$ is the variance.\n",
    "\n",
    "The below image has a few Gaussian distributions with a difference in mean (μ) and variance (σ2).\n",
    "\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/gaussians.png\"  width=400 align=center /></div>\n",
    "\n",
    "But this would only be true for a single variable. In the case of two variables, instead of a 2D bell-shaped curve, we will have a 3D bell curve as shown below:\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/gaussians-3d-166902260287820.png\"  width=400 align=center /></div>\n",
    "\n",
    "The probability density function would be given by:\n",
    "\n",
    "\n",
    "\n",
    "$$\\mathcal{N}(X|\\mu, \\Sigma)= \\frac{1}{\\sqrt{(2\\pi)|\\boldsymbol\\Sigma|}} \\exp\\left(-\\frac{1}{2}({X}-{\\mu})^T{\\boldsymbol\\Sigma}^{-1}({X}-{\\mu}) \\right)    $$ \n",
    "\n",
    "\n",
    "\n",
    "where $X$ is the input vector, μ is the 2D mean vector, and Σ is the 2×2 covariance matrix. The covariance would now define the shape of this curve. We can generalize the same for d-dimensions.\n",
    "\n",
    "Thus, this multivariate Gaussian model would have $X$ and $\\mu$ as vectors of length d, and Σ would be a *d x d* covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models\n",
    "\n",
    "Suppose there are K clusters (For the sake of simplicity here it is assumed that the number of clusters is known and it is K). So $\\mu$ and $ \\Sigma$ are also estimated for each k. Had it been only one distribution, they would have been estimated by the **maximum-likelihood method**. But since there are K such clusters and the probability density is defined as a linear function of densities of all these K distributions, i.e.\n",
    "\n",
    "$$p(\\bold{x}) =\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\bold{x}|\\mu_k, \\Sigma_k)\\Rightarrow p(x_{1},x_{2},...,x_{N}) =\\sum_{k=1}^K \\pi_k \\mathcal{N}(x_{1},x_{2},...,x_{N}|\\mu_k, \\Sigma_k)\n",
    "\\\\\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "&\\pi: \\text{mixing coefficient}\\\\\n",
    "&\\pmb{\\mu}: \\text{means}\\\\\n",
    "&\\pmb{\\Sigma}: \\text{covariance matrix}\n",
    "\\end{aligned}\n",
    "\\right.$$\n",
    "\n",
    "where $\\pi_k$   is the mixing coefficient for k-th distribution.\n",
    "\n",
    "Assuming that data points are independent, for estimating the parameters by the maximum log-likelihood method, compute $\\hspace{0.25 cm} {p( \\bold{x}|\\mu, \\Sigma, \\pi)}$.\n",
    "\n",
    "\n",
    "$$ln\\hspace{0.25 cm} {p( \\bold{x}|\\mu, \\Sigma, \\pi)} =\\sum_{n=1}^N p(x_n) =\\sum_{n=1}^N ln {\\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}  $$\n",
    "\n",
    "\n",
    "\n",
    "Now define a random variable $\\gamma_k(x_n)$ , such that $\\gamma_k(x_n)  =p(k|x_n)$.\n",
    "From Bayes’ theorem, \n",
    "\n",
    "\n",
    "$$\n",
    "\\gamma_k(x_n) =\\frac{p(x_n|k)p(k)}{\\sum_{k=1}^K p(k)p(x_n|k)} \n",
    "=\\frac{p(x_n|k)\\pi_k}{\\sum_{k=1}^K \\pi_k p(x_n|k)}\n",
    "=\\frac{\\pi_k\\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}$$\n",
    "\n",
    "Now for the log-likelihood function to be maximum, its derivative of $p(x_n|\\mu, \\Sigma, \\pi) $ with respect to $\\mu$, $\\Sigma $ and $\\pi $ should be zero. So equating the derivative of $p(x_n|\\mu, \\Sigma, \\pi) $ to zero and rearranging the terms, \n",
    "\n",
    "\n",
    "$$\\mu_k=\\frac{\\sum_{n=1}^N \\gamma_k(x_n)x_n}{\\sum_{n=1}^N \\gamma_k(x_n)}$$\n",
    "\n",
    "\n",
    "\n",
    "Similarly taking derivative with respect to $\\Sigma $ and pi respectively, one can obtain the following expressions.\n",
    "\n",
    "$$\\Sigma_k=\\frac{\\sum_{n=1}^N \\gamma_k(x_n)(x_n-\\mu_k)(x_n-\\mu_k)^T}{\\sum_{n=1}^N \\gamma_k(x_n)} \\newline $$\n",
    "\n",
    "And\n",
    "\n",
    "\n",
    "$$\\pi_k=\\frac{1}{N} \\sum_{n=1}^N \\gamma_k(x_n)$$\n",
    "\n",
    "\n",
    "\n",
    "**Note:** $\\sum_{n=1}^N\\gamma_k(x_n) $ denotes the total number of sample points in the k-th cluster. Here it is assumed that there is a total N number of samples and each sample containing d features is denoted by $x_i $.\n",
    "So it can be clearly seen that the parameters cannot be estimated in closed form. This is where the **Expectation-Maximization algorithm** is beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "The Expectation-Maximization (EM) algorithm is an iterative way to find maximum-likelihood estimates for model parameters when the data is incomplete or has some missing data points or has some hidden variables. EM chooses some random values for the missing data points and estimates a new set of data. These new values are then recursively used to estimate a better first date, by filling up missing points, until the values get fixed. \n",
    "These are the two basic steps of the EM algorithm, namely **E Step or Expectation Step or Estimation Step** and **M Step or Maximization Step**.\n",
    "\n",
    "\n",
    "- Estimation step (E step):\n",
    "  - initialize $\\mu_k   $, $\\Sigma_k   $ and $\\pi_k  $ by some random values, or by K means clustering results or by hierarchical clustering results.\n",
    "  - Then for those given parameter values, estimate the value of the latent variables (i.e $\\gamma_k(x_n)   $)\n",
    "  $$\\gamma_k(x_n) =\\frac{\\pi_k\\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}$$\n",
    "- Maximization Step(M step):\n",
    "  - Update the value of the parameters( i.e. $\\mu_k    $, $\\Sigma_k     $ and $\\pi_k    $) calculated using ML method.\n",
    "  $$\\mu_k=\\frac{\\sum_{n=1}^N \\gamma_k(x_n)x_n}{\\sum_{n=1}^N \\gamma_k(x_n)}$$\n",
    "  $$\\Sigma_k=\\frac{\\sum_{n=1}^N \\gamma_k(x_n)(x_n-\\mu_k)(x_n-\\mu_k)^T}{\\sum_{n=1}^N \\gamma_k(x_n)} \\newline $$\n",
    "  $$\\pi_k=\\frac{1}{N} \\sum_{n=1}^N \\gamma_k(x_n)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "Please finish the **Exercise** and answer **Questions**.\n",
    "### Exercise (100 Points)\n",
    "In this lab, our goal is to write a program to segment different objects using the **GMM and EM** algorithm. We also use <u>*k-means* clustering algorithm to initialize the parameters</u> of GMM. The following steps should be implemented to achieve such a goal:\n",
    "\n",
    "1. Load image\n",
    "2. Initialize parameters of GMM using K-means\n",
    "3. Implement the EM algorithm for GMM\n",
    "4. Display result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import KMeans\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "COLORS = [\n",
    "    (255, 0, 0),   # red\n",
    "    (0, 255, 0),  # green\n",
    "    (0, 0, 255),   # blue\n",
    "    (255, 255, 0), # yellow\n",
    "    (255, 0, 255), # magenta\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Image\n",
    "What you should do is to implement Z-score normalization in `load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, c = image.shape\n",
    "\n",
    "    # TODO: please normalize image_pixl using Z-score\n",
    "    _mean = None\n",
    "    _std = None\n",
    "    image_norm = None\n",
    "\n",
    "    \n",
    "    return h, w, c, image_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize means, covariance matrices and mixing coefficients of GMM\n",
    "k-means is used to initialize means, covariance matrices and mixing coefficients of GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(n_cluster, image_pixl):\n",
    "    kmeans = KMeans(n_clusters=n_cluster)# instantiate a K-means\n",
    "    labels = kmeans.fit_predict(image_pixl)# fit and get clustering result\n",
    "    initial_mus = kmeans.cluster_centers_# get centroids\n",
    "    initial_priors, initial_covs = [], []\n",
    "    #Followings are for initialization:\n",
    "    for i in range(n_cluster):\n",
    "        datas = image_pixl[labels == i, ...].T\n",
    "        initial_covs.append(np.cov(datas))\n",
    "        initial_priors.append(datas.shape[1] / len(labels))\n",
    "    return initial_mus, initial_priors, initial_covs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement GMM algorithm\n",
    "We use EM algorithm to refine GMM's parameters.\n",
    "\n",
    "Although it may be not easy for some students to derive EM formula for GMM, GMM isn't very difficult to implement once you have the formula. Therefore, to help you understand GMM more, there are still some blanks for you to fill in.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\n",
    "&\\text{E-step: }\n",
    "\\begin{aligned}\n",
    "&\\gamma(z_{nk}) = \\frac{\\pi_k\\mathcal{N}(\\bold{x}_n|\\pmb{\\mu}_k,\\pmb\\Sigma_k)}{\\sum_{j=1}^K\\pi_j\\mathcal{N}(\\bold{x}_n|\\pmb{\\mu}_j,\\pmb\\Sigma_j)}\n",
    "\\end{aligned}\n",
    "\n",
    "\\\\\n",
    "\n",
    "&\\text{M-step: }\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "&\\pmb{\\mu}_k^{new}=\\frac{1}{N_k}\\sum_{n=1}^N \\gamma(z_{nk}) \\bold{x}_n\\\\\n",
    "&\\pmb{\\Sigma}_k^{new}= \\sum_{n=1}^N \\gamma(z_{nk}) (\\bold{x}_n - \\pmb{\\mu}_k^{new})(\\bold{x}_n - \\pmb{\\mu}_k^{new})^T \\\\\n",
    "&\\pi_k^{new} = \\frac{N_k}{N}\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\n",
    "\\\\\n",
    "\n",
    "&\\text{Log likelihood: }\n",
    "\\text{ln}p(\\bold{X}|\\pmb{\\mu},\\pmb\\Sigma, \\pmb{\\pi}) = \\sum_{n=1}^N\\text{ln}\\{ \\sum_{k=1}^K\\pi_k\\mathcal{N}(\\bold{x}|\\pmb{\\mu}_k,\\pmb\\Sigma_k)\\}\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E-step\n",
    "It is in `inference()`.\n",
    "\n",
    "In the following code, `prob` is $\\pi_k\\mathcal{N}(\\bold{x}_n|\\pmb{\\mu}_k,\\pmb\\Sigma_k)$, `gamma` is  $\\gamma$. You need to implement log likelihood and $\\gamma$.\n",
    "```python\n",
    "def inference(self, datas):\n",
    "    probs = []\n",
    "    for i in range(self.ncomp):\n",
    "        mu, cov, prior = self.mus[i, :], self.covs[i, :, :], self.priors[i]\n",
    "        prob = prior * multivariate_normal.pdf(datas, mean=mu, cov=cov, allow_singular=True)\n",
    "        probs.append(np.expand_dims(prob, -1))\n",
    "    preds = np.concatenate(probs, axis=1)\n",
    "    \n",
    "    # TODO: calc log likelihood\n",
    "    log_likelihood = None\n",
    "\n",
    "    # TODO: calc gamma\n",
    "    gamma = None\n",
    "\n",
    "    return gamma, log_likelihood\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### M-step\n",
    "It is in `update()`\n",
    "\n",
    "You need to implement mean $\\mu$, covariance $\\Sigma$ and mixing coefficient $\\pi$ .\n",
    "```python\n",
    "def update(self, datas, gamma):\n",
    "    new_mus, new_covs, new_priors = [], [], []\n",
    "    soft_counts = np.sum(gamma, axis=0)\n",
    "    for i in range(self.ncomp):\n",
    "        # TODO: calc mu\n",
    "        new_mu = None\n",
    "        new_mus.append(new_mu)\n",
    "\n",
    "        # TODO: calc cov\n",
    "        new_cov = None\n",
    "        new_covs.append(new_cov)\n",
    "\n",
    "        # TODO: calc mixing coefficients\n",
    "        new_prior = None\n",
    "        new_priors.append(new_prior)\n",
    "\n",
    "    self.mus = np.asarray(new_mus)\n",
    "    self.covs = np.asarray(new_covs)\n",
    "    self.priors = np.asarray(new_priors)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration\n",
    "Iteration part is as you see in `fit()`\n",
    "``` python\n",
    "def fit(self, data, iteration):\n",
    "    prev_log_liklihood = None\n",
    "\n",
    "    bar = tqdm.tqdm(total=iteration)\n",
    "    for i in range(iteration):\n",
    "        gamma, log_likelihood = self.inference(data)\n",
    "        self.update(data, gamma)\n",
    "        if prev_log_liklihood is not None and abs(log_likelihood - prev_log_liklihood) < 1e-10:\n",
    "            break\n",
    "        prev_log_likelihood = log_likelihood\n",
    "\n",
    "        bar.update()\n",
    "        bar.set_postfix({\"log likelihood\": log_likelihood})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, ncomp, initial_mus, initial_covs, initial_priors):\n",
    "        \"\"\"\n",
    "        :param ncomp:           the number of clusters\n",
    "        :param initial_mus:     initial means\n",
    "        :param initial_covs:    initial covariance matrices\n",
    "        :param initial_priors:  initial mixing coefficients\n",
    "        \"\"\"\n",
    "        self.ncomp = ncomp\n",
    "        self.mus = np.asarray(initial_mus)\n",
    "        self.covs = np.asarray(initial_covs)\n",
    "        self.priors = np.asarray(initial_priors)\n",
    "\n",
    "    def inference(self, datas):\n",
    "        \"\"\"\n",
    "        E-step\n",
    "        :param datas:   original data\n",
    "        :return:        posterior probability (gamma) and log likelihood\n",
    "        \"\"\"\n",
    "        probs = []\n",
    "        for i in range(self.ncomp):\n",
    "            mu, cov, prior = self.mus[i, :], self.covs[i, :, :], self.priors[i]\n",
    "            prob = prior * multivariate_normal.pdf(datas, mean=mu, cov=cov, allow_singular=True)\n",
    "            probs.append(np.expand_dims(prob, -1))\n",
    "        preds = np.concatenate(probs, axis=1)\n",
    "\n",
    "        # TODO: calc log likelihood\n",
    "        log_likelihood = None\n",
    "\n",
    "        # TODO: calc gamma\n",
    "        gamma = None\n",
    "\n",
    "        return gamma, log_likelihood\n",
    "\n",
    "    def update(self, datas, gamma):\n",
    "        \"\"\"\n",
    "        M-step\n",
    "        :param datas:   original data\n",
    "        :param gamma:    gamma\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_mus, new_covs, new_priors = [], [], []\n",
    "        soft_counts = np.sum(gamma, axis=0)\n",
    "        for i in range(self.ncomp):\n",
    "            # TODO: calc mu\n",
    "            new_mu = None\n",
    "            new_mus.append(new_mu)\n",
    "\n",
    "            # TODO: calc cov\n",
    "            new_cov = None\n",
    "            new_covs.append(new_cov)\n",
    "\n",
    "            # TODO: calc mixing coefficients\n",
    "            new_prior = None\n",
    "            new_priors.append(new_prior)\n",
    "\n",
    "        self.mus = np.asarray(new_mus)\n",
    "        self.covs = np.asarray(new_covs)\n",
    "        self.priors = np.asarray(new_priors)\n",
    "\n",
    "    def fit(self, data, iteration):\n",
    "        prev_log_liklihood = None\n",
    "\n",
    "        bar = tqdm.tqdm(total=iteration)\n",
    "        for i in range(iteration):\n",
    "            gamma, log_likelihood = self.inference(data)\n",
    "            self.update(data, gamma)\n",
    "            if prev_log_liklihood is not None and abs(log_likelihood - prev_log_liklihood) < 1e-10:\n",
    "                break\n",
    "            prev_log_likelihood = log_likelihood\n",
    "\n",
    "            bar.update()\n",
    "            bar.set_postfix({\"log likelihood\": log_likelihood})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display\n",
    "We use `matplotlib` to display what we segment, you can check the code in `visualize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize(gmm, image, ncomp, ih, iw):\n",
    "    beliefs, log_likelihood = gmm.inference(image)\n",
    "    map_beliefs = np.reshape(beliefs, (ih, iw, ncomp))\n",
    "    segmented_map = np.zeros((ih, iw, 3))\n",
    "    for i in range(ih):\n",
    "        for j in range(iw):\n",
    "            hard_belief = np.argmax(map_beliefs[i, j, :])\n",
    "            segmented_map[i, j, :] = np.asarray(COLORS[hard_belief]) / 255.0\n",
    "    plt.imshow(segmented_map)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample Result\n",
    "<img src=\"images/image-20220804223008133.png\" alt=\"image-20220804223008133\" style=\"zoom:67%;\" />\n",
    "<img src=\"images/image-20220804222915979.png\" alt=\"image-20220804222915979\" style=\"zoom: 67%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions(3 points)\n",
    "1. What are the strengths of GMM; when does it perform well?\n",
    "2. What are the weaknesses of GMM; when does it perform poorly?\n",
    "3. What makes GMM a good candidate for the clustering problem, if you have enough knowledge about the data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1eb71e714c2a1bbfc91d3a1ed02399b846367c62fca82a5b3a9325d7b60709de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
