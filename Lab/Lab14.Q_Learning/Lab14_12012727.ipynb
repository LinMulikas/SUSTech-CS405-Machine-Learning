{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "Please finish the **Exercise** and answer **Questions**.\n",
    "### Exercise ( Q-Learning with Taxi-v3 üöï) (100 Points)\n",
    "\n",
    "In this exercise, you should complete the Q-learning algorithm using the Taxi-v3 environment in the gym\n",
    "\n",
    "In Taxi-v3 üöï, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, picks up the passenger, drives to the passenger‚Äôs destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "<div align=\"center\"><img src=\"images/image-20220805133926061.png\" alt=\"image-20220805133926061\" style=\"zoom:80%;\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PapFtBBzW3G"
   },
   "source": [
    "### Step 0 Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWkA0eXLq52V",
    "outputId": "a1a9e7d0-ad83-4fcf-9b2f-f72d7a168ec0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import imageio\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 1 Create Taxi-v3 üöï  environment \n",
    "Using the API imported from gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResetNeeded",
     "evalue": "Cannot call `env.render()` before calling `env.reset()`, if this is a intended action, set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResetNeeded\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/mulikas/Codes/CS405 ML/Lab/Lab14.Q_Learning/Lab14_12012727.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab14.Q_Learning/Lab14_12012727.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mTaxi-v3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab14.Q_Learning/Lab14_12012727.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m img \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrender(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab14.Q_Learning/Lab14_12012727.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(\u001b[39m'\u001b[39m\u001b[39mshow picture\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab14.Q_Learning/Lab14_12012727.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# ax = fig.add_subplot(111)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab14.Q_Learning/Lab14_12012727.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# ax.imshow(img)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab14.Q_Learning/Lab14_12012727.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# ax.set_title(\"Taxi-v3\")#ÁªôÂõæÁâáÂä†titile\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m     \u001b[39m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:47\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m\"\"\"Renders the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mResetNeeded\u001b[0m: Cannot call `env.render()` before calling `env.reset()`, if this is a intended action, set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper."
     ]
    }
   ],
   "source": [
    "env = gym.make('taxi-v3')\n",
    "img = env.render(mode='rgb_array')\n",
    "fig = plt.figure('show picture')\n",
    "\n",
    "\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(img)\n",
    "# ax.set_title(\"Taxi-v3\")#ÁªôÂõæÁâáÂä†titile\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off') # ‰∏çÊòæÁ§∫ÂàªÂ∫¶\n",
    "# plt.title(\"Taxi-v3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBOaXgtsrmtT"
   },
   "source": [
    "There are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TPNaGSZrgqA",
    "outputId": "8aa9b617-8bdd-4817-ebc2-8f97d32e9634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  500  possible states\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdeeZuokrhit",
    "outputId": "eae26ab9-5f2f-40b6-f6ee-a6bc12d57c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6  possible actions\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1r50Advrh5Q"
   },
   "source": [
    "The action space (the set of possible actions the agent can take) is discrete with **6 actions available üéÆ**:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "Reward function üí∞:\n",
    "- -1 per step unless other reward is triggered.\n",
    "- +20 delivering passenger.\n",
    "- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2  Create the Q-table and initialize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the gym api to fetch the dimension of action space and state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.n\n",
    "\n",
    "# Please complete this initialization in this line\n",
    "Q_table = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 3 Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 100000        # ‰∏ÄÂÖ±Áé©Â§öÂ∞ëÂ±ÄÊ∏∏Êàè\n",
    "total_test_episodes = 100     # ÊµãËØï‰∏≠‰∏ÄÂÖ±Ëµ∞Âá†Ê≠•\n",
    "max_steps = 99                # Max steps per episode ÊØè‰∏ÄÂ±ÄÊ∏∏ÊàèÊúÄÂ§öËµ∞Âá†Ê≠•\n",
    "\n",
    "learning_rate = 0.5           # Learning rate\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05           # Minimum exploration probability \n",
    "decay_rate = 0.008            # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "test_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
    " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
    " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 4 Q Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The formula of Q table update(Bellman equation)\n",
    "    ![Bellman equation](https://raw.githubusercontent.com/hanruihua/NoteBook/master/AI-Note/equation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = tqdm.tqdm(total=total_episodes)\n",
    "sample_rewards = []\n",
    "for episode in range(total_episodes):\n",
    "    state= env.reset()\n",
    "    step=0\n",
    "    done=False\n",
    "    sample_reward = 0 \n",
    "    while True:\n",
    "        #TODO: Please complete this action selection in this line via the maximum value\n",
    "        action = None\n",
    "        \n",
    "        # TODO:fetech the new state and reward by gym API\n",
    "        new_state, reward, done, info = None\n",
    "        # Calculate the reward of this episode\n",
    "        sample_reward += reward\n",
    "        \n",
    "        # TODO: Update the Q table \n",
    "        # Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        Q_table[state, action] = None\n",
    "        \n",
    "        # Update the state\n",
    "        state = new_state\n",
    "        \n",
    "        #store the episode reward\n",
    "        if done == True:\n",
    "            sample_rewards.append(sample_reward)\n",
    "            break\n",
    "    # Reduced exploration probability (due to decreasing uncertainty)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)         \n",
    "    # print the average reward over 1000 episodes\n",
    "    if episode%1000 == 0:\n",
    "        mean_reward = np.mean(sample_rewards)\n",
    "        sample_rewards = []\n",
    "        #print(str(episode)+\": average reward:\" + str(mean_reward))\n",
    "        bar.set_description(str(episode)+\": average reward:\" + str(mean_reward))\n",
    "    bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 5 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps=5\n",
    "bar = tqdm.tqdm(total=total_test_episodes)\n",
    "env.reset()\n",
    "rewards=[]\n",
    "images = [] \n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset(seed=test_seed[episode])\n",
    "    step = 0\n",
    "    done =False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "        #TODO:action selection\n",
    "        action = None\n",
    "        #TODO:fetech the new state and reward by gym API\n",
    "        new_state, reward, done, info = None\n",
    "        \n",
    "        total_rewards += reward\n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "     \n",
    "env.close()\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "imageio.mimsave('taxi-v3.gif', [np.array(img) for i, img in enumerate(images)], fps=fps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQoX2Iyo1OyC"
   },
   "source": [
    "###  Step 6 Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "dl7O7de11kbZ",
    "outputId": "28ec5ddc-c2f7-441c-cb9b-2c07f98fccba"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('taxi-v3.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
