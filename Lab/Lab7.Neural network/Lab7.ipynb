{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 Assignment\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12012727 王铎磊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (20 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9    10   11   12\n",
       "0  1.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  0.0\n",
       "1  0.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  0.0\n",
       "2  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  0.0\n",
       "3  1.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0\n",
       "4  1.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"datasets/digit01.csv\", header=None).astype(\"float32\")\n",
    "\n",
    "df.head()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Splitting dataset into 80% Training and 20% Testing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "Y = torch.tensor(np.array(df[12]))\n",
    "df.drop(12, axis=1, inplace=True)\n",
    "X = torch.tensor(np.array((df)))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a LogisticRegression subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a LogisticRegression subclass of nn. Module.\n",
    "########### Write Your Code Here ###########\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear = nn.Linear(12, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR(\n",
      "  (linear): Linear(in_features=12, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "model = LR()\n",
    "print(model)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "loss_fn = nn.BCELoss()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "lr = 0.01\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc= 0.5490196078431373\n",
      "acc= 0.9607843137254902\n",
      "acc= 0.9607843137254902\n",
      "acc= 0.9803921568627451\n",
      "acc= 0.9803921568627451\n",
      "acc= 0.9803921568627451\n",
      "acc= 0.9803921568627451\n",
      "acc= 0.9803921568627451\n",
      "acc= 0.9803921568627451\n",
      "acc= 1.0\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x0 = X[np.where(Y == 0)]\n",
    "x1 = X[np.where(Y == 1)]\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    # 前向传播\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # 计算 loss\n",
    "    loss = loss_fn(y_pred.squeeze(), y_train)\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    opt.step()\n",
    "\n",
    "    # 清空梯度\n",
    "    opt.zero_grad()\n",
    "    if(i % 50 == 0):\n",
    "        mask = y_pred.ge(0.5).float().squeeze()  #分类\n",
    "        correct = (mask == y_train).sum() #计算其中的分类正确的个数\n",
    "        acc = correct.item() / y_train.size(0)#计算准确率\n",
    "        print(\"acc=\",acc)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "y_predicted_cls = model(X_test).ge(0.5).float().detach().numpy()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         5\n",
      "         1.0       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        13\n",
      "   macro avg       1.00      1.00      1.00        13\n",
      "weighted avg       1.00      1.00      1.00        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "path = '../Lab7.Neural network/datasets'\n",
    "\n",
    "train = MNIST(root = path, train = True, transform=ToTensor(), download=True)\n",
    "test = MNIST(root = path, train = False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/ypvnyzt13wx0zh1tjkqvy6yh0000gn/T/ipykernel_96867/2333857998.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(train.data, dtype=torch.float32)\n",
      "/var/folders/mh/ypvnyzt13wx0zh1tjkqvy6yh0000gn/T/ipykernel_96867/2333857998.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(train.targets, dtype=torch.float32)\n",
      "/var/folders/mh/ypvnyzt13wx0zh1tjkqvy6yh0000gn/T/ipykernel_96867/2333857998.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(test.data, dtype=torch.float32)\n",
      "/var/folders/mh/ypvnyzt13wx0zh1tjkqvy6yh0000gn/T/ipykernel_96867/2333857998.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(test.targets, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(train.data, dtype=torch.float32)\n",
    "y_train = torch.tensor(train.targets, dtype=torch.float32)\n",
    "X_test = torch.tensor(test.data, dtype=torch.float32)\n",
    "y_test = torch.tensor(test.targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a MLP subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import time as t\n",
    "import numpy as np\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "########### Write Your Code Here ###########\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "path = '../Lab7.Neural network/datasets'\n",
    "\n",
    "train = MNIST(root = path, train = True, transform=ToTensor(), download=True)\n",
    "test = MNIST(root = path, train = False, transform=ToTensor())\n",
    "\n",
    "###################\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train, batch_size = 300, shuffle = True, num_workers=2, drop_last=True)\n",
    "test_iter = torch.utils.data.DataLoader(test, batch_size = 300, shuffle = True, num_workers=2, drop_last=True)\n",
    "\n",
    "#######################\n",
    "res = []\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.processing = nn.Sequential(\n",
    "            nn.Linear(28*28, 28*14),\n",
    "            nn.ReLU(28*14),\n",
    "            nn.Linear(28*14, 14*14),\n",
    "            nn.ReLU(14*14),\n",
    "            nn.Linear(14*14, 7*7)\n",
    "        )\n",
    "        \n",
    "        self.classfier = nn.Sequential(\n",
    "            nn.Linear(7*7, 16),\n",
    "            nn.Dropout(p = 0.3),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            \n",
    "            nn.Linear(16, 10),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        x = self.processing(x)\n",
    "        x = self.classfier(x)\n",
    "        return x\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此段代码谨慎运行，已经将最终模型存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Write Your Code Here ###########\n",
    "# LEARNING_RATE = 0.01\n",
    "# device = torch.device('mps')\n",
    "# model = ANN().to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# ############################################\n",
    "\n",
    "# Epochs = 1000\n",
    "# for i in range(Epochs):\n",
    "#     last_time = t.time()\n",
    "#     for cnt, (x, y) in enumerate(train_iter):\n",
    "#         samples = x.to(device)\n",
    "#         y = y.to(device)\n",
    "#         output = model(samples)\n",
    "        \n",
    "        \n",
    "#         loss = loss_fn(output, y.to(device))\n",
    "#         opt.zero_grad()\n",
    "        \n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         y_pred = torch.argmax(output, dim=1).to(device)        \n",
    "#         acc = (torch.argwhere(y_pred == y)).size(0)/y.size(0)\n",
    "#         res.append(acc)\n",
    "        \n",
    "    \n",
    "#     mean_acc = np.mean(res)\n",
    "#     res.clear()\n",
    "#     this_time = t.time()\n",
    "#     print(\"Epoch:{}/{}, step:{}, loss:{:.4f}, time:{:.2f}\".format(i + 1, Epochs, i + 1, loss.item(), this_time - last_time))\n",
    "#     print(\"acc:{:.4f}\".format(mean_acc))\n",
    "    \n",
    "    \n",
    "#     if((i + 1)%(Epochs/(Epochs/10)) == 0):\n",
    "#         torch.save(model.state_dict(), 'model_final_' + str(model)[:3] + \"_\" + str(opt)[:7] + \"lr=\" + str(LEARNING_RATE) + \"_\" + str(i/(Epochs/100) + 1) + '.pth')\n",
    "#         print(str(opt)[:7])\n",
    "#         print(\"Checkpoint save!\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/mulikas/Codes/CS405 ML/Lab/Lab7.Neural network/Lab7.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab7.Neural%20network/Lab7.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m Accuracy\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab7.Neural%20network/Lab7.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m ANN()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab7.Neural%20network/Lab7.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mAdam_lr_001999.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab7.Neural%20network/Lab7.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m checker \u001b[39m=\u001b[39m Accuracy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mulikas/Codes/CS405%20ML/Lab/Lab7.Neural%20network/Lab7.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1100\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1101\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1103\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/serialization.py:1083\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1079\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[1;32m   1080\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1083\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1084\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/serialization.py:1055\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m default_restore_location(storage, \u001b[39mstr\u001b[39;49m(map_location))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 215\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    216\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "model = ANN()\n",
    "model.load_state_dict(torch.load(\"Adam_lr_001999.pth\", map_location=torch.device(\"cuda\")))\n",
    "\n",
    "checker = Accuracy()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "res = []\n",
    "\n",
    "for testPoint in range(10):\n",
    "    acc_list = []\n",
    "    for cnt, (x, y) in enumerate(test_iter):\n",
    "        samples = x.to(device)\n",
    "        output = model(samples)\n",
    "\n",
    "        y = y.to(device)\n",
    "        y_pred = torch.argmax(output, dim=1).to(device)        \n",
    "        acc = (torch.argwhere(y_pred == y)).size(0)/y.size(0)\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "    \n",
    "    res.append(np.mean(acc_list))\n",
    "\n",
    "\n",
    "print(np.mean(res))\n",
    "#0.9793636363636363\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 3,  ..., 4, 6, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_cls = torch.argmax(model(X_test), dim=1)\n",
    "print(X_test.size())\n",
    "y_predicted_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Questions (10 points )\n",
    "1.What's the difference between logistic regression and Perceptron?\n",
    "\n",
    "2.Advantages and disadvantages of neural networks?\n",
    "\n",
    "3.What is the role of Activation Function in Neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 逻辑回归可以看做最简单的感知器，而感知器可以包括很多其他模型。\n",
    "2. 处理大样本数据的时候，潜力更大，方便进行复用和转移。训练成本更高，解释性不够强。\n",
    "3. 用来把数据映射到 (0, 1) 内，并且让靠近边界的样本移动到两端。有利于分类。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
